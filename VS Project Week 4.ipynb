{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc875f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline   \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.stats.proportion as smp\n",
    "import statsmodels.stats.multitest as smm\n",
    "import statsmodels.stats.weightstats as smw\n",
    "import statsmodels.stats.diagnostic as smd\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd73806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data for modeling\n",
    "def preprocess_data(df):    \n",
    "    # Define features and target variable\n",
    "    X = df.drop(columns=['Customer_ID', 'Customer_Churn', 'Defaulted'])\n",
    "    y = df['Customer_Churn']\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessing pipelines for numeric and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return X, y, preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a logistic regression model\n",
    "def train_logistic_regression(X, y, preprocessor):\n",
    "    # Create a pipeline with preprocessing and logistic regression\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    model_pipeline.fit(X, y)\n",
    "    \n",
    "    return model_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X, y):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42183523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Import Necessary Libraries ---\n",
    "%pip install --upgrade scikit-learn>=1.3.0 causalml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for modeling and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, r2_score, mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Advanced Models\n",
    "import xgboost as xgb\n",
    "\n",
    "# CausalML for Uplift Modeling\n",
    "from causalml.inference.meta import SLearner\n",
    "from causalml.metrics import plot_uplift_by_percentile\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# --- 2. Load and Prepare the Dataset ---\n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# Drop the Customer_ID as it's just an identifier\n",
    "df = df.drop('Customer_ID', axis=1)\n",
    "\n",
    "# Display basic info to confirm data types and no nulls\n",
    "print(\"--- Initial Data Info ---\")\n",
    "print(df.info())\n",
    "\n",
    "# Note: The dataset has weird repeating decimals in some columns (e.g., 84398.0555...),\n",
    "# which are likely the result of mean imputation for missing values. For this exercise,\n",
    "# we'll proceed, but in a real-world scenario, this would warrant further investigation.\n",
    "\n",
    "# --- 3. Feature Engineering and Preprocessing Setup ---\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# The target variables should not be in the feature list\n",
    "target_vars = ['Customer_Churn', 'Defaulted', 'Sales']\n",
    "for var in target_vars:\n",
    "    if var in numerical_features:\n",
    "        numerical_features.remove(var)\n",
    "\n",
    "# Create a preprocessing pipeline for reuse\n",
    "# 1. OneHotEncode categorical features\n",
    "# 2. Scale numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep other columns (like targets) untouched for now\n",
    ")\n",
    "\n",
    "print(\"\\nNumerical Features:\", numerical_features)\n",
    "print(\"Categorical Features:\", categorical_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
